{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** If you are running on Binder **\n",
    "\n",
    "You should:\n",
    "- Launch a new Desktop tab by going to New -> Desktop in the index page ([screenshot](../_images/binder-desktop.png))\n",
    "\n",
    "- then uncomment and execute the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['DISPLAY'] = ':1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%gui qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "--------------\n",
    "\n",
    "## Separating an image into one or more regions of interest.\n",
    "\n",
    "Everyone has heard or seen Photoshop or a similar graphics editor take a person from one image and place them into another.  The first step of doing this is *identifying where that person is in the source image*.\n",
    "\n",
    "In popular culture, the Terminator's vision segments humans:\n",
    "\n",
    "<img src=\"../workshops/archived/2014-scipy/images/terminator-vision.png\" width=\"700px\"/>\n",
    "\n",
    "### Segmentation contains two major sub-fields\n",
    "\n",
    "* **Supervised** segmentation: Some prior knowledge, possibly from human input, is used to guide the algorithm.  Supervised algorithms currently included in scikit-image include\n",
    "  *  Thresholding algorithms which require user input (`skimage.filters.threshold_*`)\n",
    "  * `skimage.segmentation.random_walker`\n",
    "  * `skimage.segmentation.active_contour`\n",
    "* **Unsupervised** segmentation: No prior knowledge.  These algorithms attempt to subdivide into meaningful regions automatically.  The user may be able to tweak settings like number of regions.\n",
    "  *  Thresholding algorithms which require no user input.\n",
    "  * `skimage.segmentation.slic`\n",
    "  * `skimage.segmentation.watershed`\n",
    "  * `skimage.segmentation.chan_vese`\n",
    "  * `skimage.segmentation.felzenszwalb`\n",
    "  * `skimage.segmentation.quickshift`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some standard imports and a helper function to display our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import napari\n",
    "\n",
    "import skimage.data as data\n",
    "import skimage.segmentation as seg\n",
    "from skimage import filters\n",
    "from skimage import draw\n",
    "from skimage import color\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding\n",
    "\n",
    "In some images, global or local contrast may be sufficient to separate regions of interest.  Simply choosing all pixels above or below a certain *threshold* may be sufficient to segment such an image.\n",
    "\n",
    "Let's try this on an image of a textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data.page()\n",
    "\n",
    "viewer = napari.view_image(text);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "A histogram simply plots the frequency (number of times) values within a certain range appear against the data values themselves.  It is a powerful tool to get to know your data - or decide where you would like to threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(text.ravel(), bins=256, range=[0, 255])\n",
    "ax.set_xlim(0, 256);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation: supervised thresholding\n",
    "\n",
    "Try simple NumPy methods and a few different thresholds on this image.  Because *we* are setting the threshold, this is *supervised* segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_segmented = ... # your code here\n",
    "\n",
    "viewer.add_labels(text_segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not ideal results!  The shadow on the left creates problems; no single global value really fits.\n",
    "\n",
    "What if we don't want to set the threshold every time?  There are several published methods which look at the histogram and choose what should be an optimal threshold without user input.  These are unsupervised.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation: unsupervised thresholding\n",
    "\n",
    "Here we will experiment with a number of automatic thresholding methods available in scikit-image.  Because these require no input beyond the image itself, this is *unsupervised* segmentation.\n",
    "\n",
    "These functions generally return the threshold value(s), rather than applying it to the image directly.\n",
    "\n",
    "Try `otsu` and `li`, then take a look at `local` or `sauvola`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_auto_threshold = filters.threshold_  # Hit tab with the cursor after the underscore, try several methods\n",
    "\n",
    "viewer.add_labels(text_auto_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised segmentation\n",
    "\n",
    "Thresholding can be useful, but is rather basic and a high-contrast image will often limit its utility.  For doing more fun things - like removing part of an image - we need more advanced tools.\n",
    "\n",
    "For this section, we will use the `astronaut` image and attempt to segment Eileen Collins' head using supervised segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our source image\n",
    "astronaut = data.astronaut()\n",
    "viewer = napari.view_image(astronaut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contrast is pretty good in this image for her head against the background, so we will simply convert to grayscale with `rgb2gray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astronaut_gray = color.rgb2gray(astronaut)\n",
    "viewer.add_image(astronaut_gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use two methods, which segment using very different approaches:\n",
    "\n",
    "* **Active Contour**: Initializes using a user-defined contour or line, which then is attracted to edges and/or brightness.  Can be tweaked for many situations, but mixed contrast may be problematic.\n",
    "* **Random walker**: Initialized using any labeled points, fills the image with the label that seems least distant from the origin point (on a path weighted by pixel differences).  Tends to respect edges or step-offs, and is surprisingly robust to noise.  Only one parameter to tweak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active contour segmentation\n",
    "\n",
    "We must have a set of initial parameters to 'seed' our segmentation this.  Let's draw a circle around the astronaut's head to initialize the snake.\n",
    "\n",
    "This could be done interactively, with a GUI, but for simplicity we will start at the point [100, 220] and use a radius of 100 pixels.  Just a little trigonometry in this helper function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_points(resolution, center, radius):\n",
    "    \"\"\"\n",
    "    Generate points defining a circle on an image.\n",
    "    \"\"\"\n",
    "    radians = np.linspace(0, 2*np.pi, resolution)\n",
    "    r = center[0] + radius*np.sin(radians)\n",
    "    c = center[1] + radius*np.cos(radians)\n",
    "    \n",
    "    return np.stack([r, c], axis=1)\n",
    "\n",
    "# Exclude last point because a closed path should not have duplicate points\n",
    "outer_points = circle_points(200, [100, 220], 100)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = seg.active_contour(\n",
    "    astronaut_gray,\n",
    "    outer_points,\n",
    "    coordinates='rc',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_shapes(\n",
    "    [outer_points],\n",
    "    name='initial points',\n",
    "    edge_color='red',\n",
    "    face_color=[(0,0,0,0)],\n",
    "    edge_width=2,\n",
    "    shape_type='polygon'\n",
    ")\n",
    "viewer.add_shapes(\n",
    "    [snake],\n",
    "    name='active contour',\n",
    "    edge_color='blue',\n",
    "    face_color=[(0,0,0,0)],\n",
    "    edge_width=2,\n",
    "    shape_type='polygon'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't do much!\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Go back and read the documentation of `seg.active_contour`, and change some parameters to see whether you can get a good contour. \n",
    "\n",
    "*Tip:* change parameters logarithmically (10, 1, 0.1, 0.01, ...) to make sure you are in the right order of magnitude. *Then* refine linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random walker\n",
    "\n",
    "One good analogy for random walker uses graph theory.  \n",
    "\n",
    "* The distance from each pixel to its neighbors is weighted by how similar their values are; the more similar, the lower the cost is to step from one to another\n",
    "* The user provides some seed points\n",
    "* The algorithm finds the cheapest paths from each point to each seed value.  \n",
    "* Pixels are labeled with the cheapest/lowest path.\n",
    "\n",
    "We will re-use the seed values from our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astronaut_seeds = np.zeros(astronaut_gray.shape, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random walker algorithm expects a label image as input.  Any label above zero will be treated as a seed; all zero-valued locations will be filled with labels from the positive integers available.\n",
    "\n",
    "There is also a masking feature where anything labeled -1 will never be labeled or traversed, but we will not use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = draw.circle_perimeter(100, 220, 25)\n",
    "\n",
    "astronaut_seeds[inner] = 1\n",
    "astronaut_seeds[tuple(outer_points.astype(int).T)] = 2\n",
    "\n",
    "viewer.add_labels(astronaut_seeds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astronaut_rw = seg.random_walker(astronaut_gray, astronaut_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_labels(astronaut_rw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Again, the default random walker parameters have let us down! Look at the documentation for `seg.random_walker` and play with the parameters to segment the astronaut's head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised segmentation\n",
    "\n",
    "Sometimes, human input is not possible or feasible - or, perhaps your images are so large that it is not feasible to consider all pixels simultaneously.  Unsupervised segmentation can then break the image down into several sub-regions, so instead of millions of pixels you have tens to hundreds of regions.\n",
    "\n",
    "### SLIC\n",
    "\n",
    "There are many analogies to machine learning in unsupervised segmentation.  Our first example directly uses a common machine learning algorithm under the hood - K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIC works in color, so we will use the original astronaut\n",
    "astronaut_slic = seg.slic(astronaut, start_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label2rgb replaces each discrete label with the average interior color\n",
    "viewer.add_labels(astronaut_slic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "The results show some promise, but there are still leaks between real regions. As before, tweak the parameters until the segmentation contains the astronaut's head. It's ok if more than one region contains the head, but background regions should not \"bleed into\" the head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've reduced this image from 512 $\\times$ 512 = 262,000 pixels down to 100 regions. This makes it much easier to finish the segmentation interactively, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watershed\n",
    "\n",
    "Watershed segmentation requires an image to have high values in between different objects, and low within the objects. *What does this remind you of?*\n",
    "\n",
    "To understand watershed, think of the image as a landscape. When a drop lands somewhere on the image, it rolls down the image landscape to a low point. The set of points in the image that all land in the same point is called a *watershed basin*.\n",
    "\n",
    "As a smoothing technique, the watershed method is usually *seeded* with markers, and basins can then only belong to one of the given markers. Basins that don't have a marker end up merged to a neighboring basin with the lowest separating ridge.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Use `segmentation.watershed` to generate a segmentation of the `data.coins` images. You may need to preprocess the image first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
